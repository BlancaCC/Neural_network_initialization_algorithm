\section*{Further works}
\addcontentsline{toc}{section}{
    \protect\numberline{}Further work
}

\subsection*{Classification problems} 
Even though, as neurons are increased the error decreased,
for classification problems where the output should be an exact class the Neural Network should be compose by a classification function. 

Add photo. 


\subsection*{Symbolic equation for multilayer neural networks}

The equations could be generalized for any neural architecture by symbolic calcs. 

\subsection*{Reducing noise for tails: Runge's phenomenon}

For no asymptotic activation functions the internode values $x$ between the 
selected values $x_k, x_{k+1}$

\begin{equation}
    p \cdot x_k  
    < p \cdot x
    p \cdot x_{k+1} 
\end{equation}

the higher is $k$, more neurons will be activated  by $x$ and the higher and less smoothed will be its predictions. 
It is similar to the Runge's phenomenon, because essentially we are doing an interpolation. 

This phenomena increase the error and some algorithm modification may be needed. 
Reducing the error by assign two neuron per node, one is an auxiliar one to counter the 
growing of the neurons. 
Change the paradigm? Some coeefficient corrector?

